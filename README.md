# RecomApi
Реализация серверной части рекомендательной системы на основе оценок пользователя и текстового описания продукта

## Запуск

- [api-compose.yml](api-compose.yml) - для запуска api
- [bot-compose.yml](bot-compose.yml) - для запуска телеграмм бота

ВНИМАНИЕ! При запуске контейнера происходит скачивание зависимостей, а их на 10GB,
поэтому запуск может занять некоторое время и большое количество места на диске.

Для запуска проекта необходимо выполнить команду:
bash
docker-compose -f "название контейнера" build
docker-compose -f "название контейнера" up


api будет доступно по адресу: http://localhost:60106

## Постановка задачи

Было решено реализовать рекомендательную систему.

На основе тех предметов, которые пользователь оценит, как понравившиеся и не понравившиеся.
Учитывая эти данные система будет рекомендовать пользователю предметы, которые ему могут понравиться.

Так же, система будет основываться на векторном представлении текстового описания предметов, поскольку это не требует
построения сложных систем тегов или жанров предметов и их обработки.
Что поможет связать разные по форме предметы, но одинаковые по смыслу.

## Реализация

### База данных

Для хранения данных была выбрана база данных PostgreSQL, и python библиотека для работы с ней SQLAlchemy.

Как тестовая база данных была взята база данных кинопоиска, из которой были взяты фильмы с дополнительным коротким
описанием ~ 11000 фильмов.

Как основные характеристики были взяты:
- название
- фото
- длинное описание

они хранятся в таблице фильмов

Остальные характеристики были помещены в отдельную таблицу, где каждая характеристика храниться в отдельной строке

Так же били созданы таблицы векторов, коротких описаний и лемматизированных коротких описаний, поскольку процессы
по их созданию довольно долгие и их не стоит делать каждый раз при запросе пользователя.

Все модели описаны в файле [Db_objects.py](database%2FDb_objects.py)

### Обработка данных

Для обработки данных были использованы библиотеки:
- pymystem3 - для лемматизации текста
- nltk - для очисти текста от стоп слов
- sentence_transformers - для создания векторов текста

При старте проекта проверяется флаг из конфига, если он True, то происходит парсинг и обработка данных из базы данных.

Обработка данных происходит в несколько этапов:
1. Лемматизация текста
2. Очистка текста от стоп слов
3. Векторизация текста

Лемматизация и очистка текста реализованна в [lemmatization.py](database%2FNLP%2Flemmatization.py)

Обработка и векторизация текста реализованна в [processing.py](database%2FNLP%2Fprocessing.py)

### Рекомендательная система

В ходе исследования было решено использовать ансамбль решающих деревьев, поскольку он показал лучшие результаты
по сравнению с косинусном расстоянием.

Для ансамбля была использована библиотека CatBoost, поскольку она показывает хорошие результаты на небольших данных,
при этом имея большую скорость работы.

При каждом запросе пользователя, происходит поиск в базе данных событий, которые пользователь оценил как понравившиеся
и не понравившиеся.
Из базы данных берутся векторы коротких описаний событий, а так же остальные данные для увеличения точности
предсказаний, для фильмов из кинописка это были:
- жанры (в виде вектора)
- продолжительность
- год выпуска
- оценки критиков и пользователей

Дынные подаются на вход CatBoostClassifier, модель обучается под данного пользователя, после предсказывает оценку
пользователя для каждого события.
После чего события сортируются по убыванию вероятности что понравится пользователю.
И возвращаться пользователю первые 5.

Реализация рекомендательной системы находиться в [recomindation_alg.py](database%2Frecomindation_alg.py)

### Логика работы

Логика работы рекомендательной системы описана в [async_db.py](database%2Fasync_db.py)

Здесь реализованна логика работы с базой данных, а так же логика работы с рекомендательной системой.
Все выполнено в SQLAlchemy в асинхронном режиме.

Работа с базой данных выполнена через обращения к статичным методам класса asyncHandler.

При каждом вызове функции, которая обращается к базе данных, происходит подключение к базе данных, через декоратор
@Session и после выполнения функции, происходит отключение от базы данных.

### Docker

Для запуска проекта в docker контейнере, были написаны два Docker-compose файла:
- [api-compose.yml](api-compose.yml) - для запуска api
- [bot-compose.yml](bot-compose.yml) - для запуска телеграмм бота

В них происходит запуск контейнера с базой данных, а так же контейнера с api или ботом.

## Документация

Документация к api находиться по адресу http://localhost:60106/docs (более полная и интерактивная) или
в файле [doc.md](api%2Fdoc.md)

## Результаты рекомендательной сети

Для оценки качества рекомендательной сети была использована метрика PRAUC(Precision-Recall AUC).

PRAUC - площадь под кривой точности и полноты. Эта метрика показывает качество ранжирования положительных классов,
Эта метрика подходит в рамках этой задачи, поскольку мы ранжируем события, которые пользователь оценил бы как
понравившиеся, а не понравившиеся нас не интересуют.

Для тестового датасета были выбраны фильмы про 1-2 мировую войну в качестве положительных событий, а в качестве
отрицательных событий были выбраны случайные фильмы не про войну.

Тестирование представлено в [stat_jp.ipynb](statistic%2Fstat_jp.ipynb) (надо запустить jupyter notebook
поскольку графики CartBoost не сохраняются в jupyter)

Результаты тестирования:

1. Для тестового датасета были использованы только векторы коротких описаний

![img1.png](statistic%2Fimg1.png)

2. Для тестового датасета были использованы векторы коротких описаний и остальные данные о фильмах

![img2.png](statistic%2Fimg2.png)

Из-за того что модель при обучении использует случайные изначальные веса, точность 1 варианта не всегда меньше чем
во 2 варианте, но в среднем 2 вариант показывает лучшие результаты.

Точность на тестовой выборке в среднем 0.8 - 0.9, что является хорошим результатом.
